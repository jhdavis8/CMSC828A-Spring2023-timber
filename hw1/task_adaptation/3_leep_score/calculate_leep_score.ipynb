{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_nli (/fs/classhomes/spring2023/cmsc828a/c828a017/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.99it/s]\n",
      "Loading cached processed dataset at /fs/classhomes/spring2023/cmsc828a/c828a017/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0e2f950f890e5695.arrow\n",
      "Loading cached processed dataset at /fs/classhomes/spring2023/cmsc828a/c828a017/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-c75e104972489eea.arrow\n",
      "Loading cached processed dataset at /fs/classhomes/spring2023/cmsc828a/c828a017/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-fcf01ce179d8a164.arrow\n",
      "Loading cached processed dataset at /fs/classhomes/spring2023/cmsc828a/c828a017/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-15306bd64c5bd8e0.arrow\n",
      "Some weights of the model checkpoint at ../1_finetune_for_MRC were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ../1_finetune_for_MRC and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "model_name_or_path = '../1_finetune_for_MRC'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "metric = evaluate.load(\"glue\", 'mnli')\n",
    "\n",
    "def load_target_dataset():\n",
    "    raw_datasets = load_dataset(\"multi_nli\")\n",
    "    train_dataset = raw_datasets['train'].filter(lambda x: x[\"genre\"] == \"slate\" or x[\"genre\"] == \"travel\")\n",
    "    train_dataset = train_dataset.train_test_split(train_size=0.01) # this is here for preprocessing labels\n",
    "\n",
    "    raw_datasets = DatasetDict({\n",
    "        'train': train_dataset['train'],\n",
    "        'validation_matched': raw_datasets['validation_matched'].filter(lambda x: x['genre'] == 'slate' or x['genre'] == 'travel'),\n",
    "        'validation_mismatched': raw_datasets['validation_mismatched'],\n",
    "    })\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # this is set up for MNLI, taken from run_glue_no_trainer\n",
    "        sentence1_key = 'premise'\n",
    "        sentence2_key = 'hypothesis'\n",
    "\n",
    "        # Tokenize the texts\n",
    "        texts = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*texts, padding=False, max_length=128, truncation=True)\n",
    "        result[\"labels\"] = examples[\"label\"]\n",
    "        return result\n",
    "\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    target_dataset = processed_datasets['validation_matched']\n",
    "    target_dataset = target_dataset.train_test_split(test_size=0.010)['test'] # TODO for dev\n",
    "    return target_dataset\n",
    "\n",
    "def load_pretrained_model_mnli():\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path, num_labels=3, finetuning_task='mnli')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_pretrained_model_squad():\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "target_dataset = load_target_dataset() # this is target datset D in the leep paper\n",
    "model = load_pretrained_model_mnli() # this is theta in the leep paper, pretrained on some other dataset\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# step 1: obtain dummy distribution \\theta(x_i)\n",
    "target_label_space = [0, 1, 2] # Y\n",
    "source_label_space = [0, 1, 2] # Z\n",
    "ground_truths = np.array(target_dataset['labels']) # y_i\n",
    "n = len(target_dataset)\n",
    "\n",
    "target_dataloader = DataLoader(target_dataset, collate_fn=DataCollatorWithPadding(tokenizer), batch_size=8)\n",
    "predictions = []\n",
    "\n",
    "for step, batch in enumerate(target_dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_outputs = model(**batch)\n",
    "        batch_predictions = batch_outputs.logits.argmax(dim=-1)\n",
    "        predictions += batch_predictions\n",
    "\n",
    "label, count = np.unique(predictions, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3, 1: 25, 2: 12}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(*(label, count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.075, 1: 0.625, 2: 0.3}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_distribution = {z_i: 0 for z_i in source_label_space} # initialize all label probabilities to 0 s.t. if it's not predicted, it still shows up\n",
    "dummy_distribution.update({label[i]: count[i]/n for i in range(len(label))})\n",
    "dummy_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.03,\n",
       " (0, 1): 0.25,\n",
       " (0, 2): 0.12,\n",
       " (1, 0): 0.0225,\n",
       " (1, 1): 0.1875,\n",
       " (1, 2): 0.09,\n",
       " (2, 0): 0.0225,\n",
       " (2, 1): 0.1875,\n",
       " (2, 2): 0.09}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2.1: compute empirical joint distribution P(y, z)\n",
    "empirical_joint_distribution = {}\n",
    "\n",
    "for y_i in target_label_space:\n",
    "    for z_i in source_label_space:\n",
    "        empirical_joint_distribution[(y_i, z_i)] = dummy_distribution[z_i]*sum(ground_truths == y_i)/n\n",
    "\n",
    "empirical_joint_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.4, 1: 0.3, 2: 0.3}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2.2: compute empirical marginal distribution P(z)\n",
    "empirical_marginal_distribution = {y: 0 for y in target_label_space}\n",
    "\n",
    "for y_i in target_label_space:\n",
    "    for z_i in source_label_space:\n",
    "        empirical_marginal_distribution[y_i] += empirical_joint_distribution[y_i, z_i]\n",
    "\n",
    "empirical_marginal_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.075,\n",
       " (0, 1): 0.8333333333333334,\n",
       " (0, 2): 0.4,\n",
       " (1, 0): 0.056249999999999994,\n",
       " (1, 1): 0.625,\n",
       " (1, 2): 0.3,\n",
       " (2, 0): 0.056249999999999994,\n",
       " (2, 1): 0.625,\n",
       " (2, 2): 0.3}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2.3: compute empirical conditional distribution P(y|z)\n",
    "empirical_conditional_distribution = {(y,z): empirical_joint_distribution[(y, z)]/empirical_marginal_distribution[z] for y in target_label_space for z in source_label_space}\n",
    "empirical_conditional_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6088557759186706"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3: compute LEEP score\n",
    "T = 1/n*sum([np.log(sum([empirical_conditional_distribution[(y_i, z)]*dummy_distribution[z] for z in source_label_space])) for y_i in ground_truths])\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e64590719278c68410a162e85160ce787d85035fe49aebed1ca40f8be12a43c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
